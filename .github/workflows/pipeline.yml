name: tourism-mlops-pipeline

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch: {}
  schedule:
    - cron: "0 3 * * 1"

permissions:
  contents: write
  id-token: write

concurrency:
  group: tourism-mlops
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.10"
  HF_TOKEN:     ${{ secrets.HF_TOKEN }}
  # Prefer Variables; if missing, fall back to Secrets of same name
  DATASET_REPO: ${{ vars.DATASET_REPO != '' && vars.DATASET_REPO || secrets.DATASET_REPO }}
  MODEL_REPO:   ${{ vars.MODEL_REPO   != '' && vars.MODEL_REPO   || secrets.MODEL_REPO }}
  SPACE_ID:     ${{ vars.SPACE_ID }}
  HF_USERNAME:  ${{ vars.HF_USERNAME != '' && vars.HF_USERNAME || secrets.HF_USERNAME }}

jobs:
  train-register-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Debug repo ids
        run: |
          echo "DATASET_REPO=${{ env.DATASET_REPO }}"
          echo "MODEL_REPO=${{ env.MODEL_REPO }}"
          echo "SPACE_ID=${{ env.SPACE_ID }}"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r requirements.txt

      - name: Verify required inputs
        run: |
          test -n "${{ secrets.HF_TOKEN }}"   || (echo "HF_TOKEN missing" && exit 1)
          test -n "${{ env.DATASET_REPO }}"   || (echo "DATASET_REPO missing (set as Variable or Secret)" && exit 1)
          test -n "${{ env.MODEL_REPO }}"     || (echo "MODEL_REPO missing (set as Variable or Secret)" && exit 1)
          echo "All required inputs present."

      - name: Sanity check HF auth & dataset access
        env:
          HF_TOKEN:     ${{ secrets.HF_TOKEN }}
          DATASET_REPO: ${{ env.DATASET_REPO }}
        run: |
          python - <<'PY'
          import os, sys
          from huggingface_hub import HfApi, RepositoryNotFoundError
          tok  = os.environ["HF_TOKEN"].strip()
          repo = os.environ["DATASET_REPO"].strip()
          api  = HfApi(token=tok)
          print("Checking dataset repo:", repo)
          try:
            files = api.list_repo_files(repo_id=repo, repo_type="dataset")
          except RepositoryNotFoundError:
            print(f"ERROR: Dataset repo not found: {repo}")
            sys.exit(1)
          need = {"data/train.csv", "data/test.csv"}
          missing = need.difference(files)
          if missing:
            print("ERROR: Missing files in dataset:", ", ".join(sorted(missing)))
            sys.exit(1)
          print("Dataset OK. File count:", len(files))
          PY

      - name: Train, Evaluate, Register model on HF Hub
        env:
          HF_TOKEN:              ${{ secrets.HF_TOKEN }}
          HUGGINGFACE_HUB_TOKEN: ${{ secrets.HF_TOKEN }}
          DATASET_REPO:          ${{ env.DATASET_REPO }}
          MODEL_REPO:            ${{ env.MODEL_REPO }}
        run: |
          python - <<'PY'
          import os, json, datetime, pathlib, joblib
          import pandas as pd
          from datasets import load_dataset
          from sklearn.compose import ColumnTransformer
          from sklearn.pipeline import Pipeline
          from sklearn.preprocessing import OneHotEncoder
          from sklearn.impute import SimpleImputer
          from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score, brier_score_loss
          import xgboost as xgb
          from huggingface_hub import create_repo, upload_folder, HfApi

          HF_TOKEN    = os.environ["HF_TOKEN"]
          DATASET_REPO= os.environ["DATASET_REPO"]
          MODEL_REPO  = os.environ["MODEL_REPO"]

          ds = load_dataset("csv", data_files={
              "train": f"hf://datasets/{DATASET_REPO}/data/train.csv",
              "test":  f"hf://datasets/{DATASET_REPO}/data/test.csv",
          })
          train = ds["train"].to_pandas()
          test  = ds["test"].to_pandas()

          for junk in ["Unnamed: 0", "index"]:
              if junk in train.columns: train = train.drop(columns=[junk])
              if junk in test.columns:  test  = test.drop(columns=[junk])

          y = "ProdTaken"
          Xtr, ytr = train.drop(columns=[y]), train[y]
          Xte, yte = test.drop(columns=[y]),  test[y]

          cat = [c for c in ["TypeofContact","CityTier","Occupation","Gender","MaritalStatus","Designation","ProductPitched"] if c in Xtr.columns]
          num = [c for c in Xtr.columns if c not in cat and c!="CustomerID"]

          prep = ColumnTransformer([
              ("num", SimpleImputer(strategy="median"), num),
              ("cat", Pipeline(steps=[
                  ("imputer", SimpleImputer(strategy="most_frequent")),
                  ("ohe", OneHotEncoder(handle_unknown="ignore"))
              ]), cat)
          ])

          model = xgb.XGBClassifier(
              objective="binary:logistic", eval_metric="logloss",
              n_estimators=400, learning_rate=0.1, max_depth=6,
              subsample=0.9, colsample_bytree=0.9, tree_method="hist",
              random_state=42
          )
          pipe = Pipeline([("prep", prep), ("clf", model)])
          pipe.fit(Xtr, ytr)

          proba = pipe.predict_proba(Xte)[:,1]
          pred  = (proba >= 0.5).astype(int)
          metrics = {
              "roc_auc":  float(roc_auc_score(yte, proba)),
              "pr_auc":   float(average_precision_score(yte, proba)),
              "f1":       float(f1_score(yte, pred)),
              "precision":float(precision_score[yte == 1], default=0.0) if hasattr(__import__('numpy'), 'array') else float('nan'),
              "recall":   float(recall_score(yte, pred)),
              "brier":    float(brier_score_loss(yte, proba)),
          }
          print("Metrics:", metrics)

          out = pathlib.Path("export_model"); out.mkdir(exist_ok=True)
          joblib.dump(pipe, out / "model.joblib")
          with open(out / "metadata.json", "w") as f:
              json.dump({"metrics":metrics, "feature_order": list(Xtr.columns), "created":str(datetime.datetime.utcnow())}, f, indent=2)

          api = HfApi(token=HF_TOKEN)
          try:
              create_repo(repo_id=MODEL_REPO, repo_type="model", private=False, token=HF_TOKEN)
          except Exception:
              pass
          upload_folder(folder_path=str(out), repo_id=MODEL_REPO, repo_type="model", token=HF_TOKEN)
          print("Uploaded model:", f"https://huggingface.co/{MODEL_REPO}")

          with open("metrics_summary.json","w") as f:
              json.dump({"model_repo": MODEL_REPO, **metrics}, f, indent=2)
          PY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-and-metrics
          path: |
            export_model/
            metrics_summary.json

      - name: (Optional) Redeploy HF Space
        if: ${{ env.SPACE_ID != '' }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SPACE_ID: ${{ env.SPACE_ID }}
        run: |
          python - <<'PY'
          from huggingface_hub import HfApi
          import os
          api = HfApi(token=os.environ["HF_TOKEN"])
          api.upload_folder(
              folder_path="tourism_project/deployment",
              repo_id=os.environ["SPACE_ID"],
              repo_type="space",
              commit_message="Auto-deploy from GitHub Actions"
          )
          print("Redeployed Space:", f"https://huggingface.co/spaces/{os.environ['SPACE_ID']}")
          PY

      - name: Commit metrics summary back to main
        if: ${{ github.ref == 'refs/heads/main' }}
        run: |
          git add metrics_summary.json || true
          git commit -m "CI: update metrics summary [skip ci]" || echo "No changes to commit"
          git push origin main || true
